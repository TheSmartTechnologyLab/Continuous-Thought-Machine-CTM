{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codewithdark-git/Continuous-Thought-Machine-CTM/blob/main/01_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a72c0e",
      "metadata": {
        "id": "04a72c0e"
      },
      "source": [
        "# The Continuous Thought Machine – Tutorial 01: MNIST [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/codewithdark-git/Continuous-Thought-Machine-CTM/blob/main/01_mnist.ipynb) [![arXiv](https://img.shields.io/badge/arXiv-2505.05522-b31b1b.svg)](https://arxiv.org/abs/2505.05522)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88fc6d1",
      "metadata": {
        "id": "d88fc6d1"
      },
      "source": [
        "Modern deep learning models ignore time as a core computational element. In contrast, the **Continuous Thought Machine (CTM)** introduces internal recurrence and neural synchronization to model *thinking as a temporal process*.\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "- **Internal Ticks**: The CTM runs over a self-generated temporal axis (independent of input), which we via as a dimension over which though can unfold.\n",
        "- **Neuron-Level Models**: Each neuron has a private MLP that processes its own history of pre-activations over time.\n",
        "- **Synchronization as Representation**: CTMs compute neuron-to-neuron synchronization over time and use these signals for attention and output.\n",
        "\n",
        "### Why It Matters\n",
        "\n",
        "- Enables **interpretable, dynamic reasoning**\n",
        "- Supports **adaptive compute** (e.g. more ticks for harder tasks)\n",
        "- Works across tasks: classification, reasoning, memory, RL—*without changing the core mechanisms*.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05cf27b",
      "metadata": {
        "id": "b05cf27b"
      },
      "source": [
        "### MNIST Classification\n",
        "\n",
        "In this tutorial we walk through a simple example; training a CTM to classify MNIST digits. We cover:\n",
        "- Defining the model\n",
        "- Constructing the loss function\n",
        "- Training\n",
        "- Building vizualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c257dbd3",
      "metadata": {
        "id": "c257dbd3"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b626e031",
      "metadata": {
        "id": "b626e031",
        "outputId": "67caea70-3670-4982-ea70-0a3d22f1e88b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapy\n",
            "  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from mediapy) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapy) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapy) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mediapy) (11.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->mediapy) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapy) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.17.0)\n",
            "Downloading mediapy-1.2.4-py3-none-any.whl (26 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, mediapy\n",
            "Successfully installed jedi-0.19.2 mediapy-1.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7bfbfe0",
      "metadata": {
        "id": "a7bfbfe0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import math\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import mediapy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee6aa63",
      "metadata": {
        "id": "5ee6aa63"
      },
      "source": [
        "We start by defining some helper classes, which we will use in the CTM.\n",
        "\n",
        "Of note is the SuperLinear class, which implements N unique linear transformations. This SuperLinear class will be used for the neuron-level models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "50c9ac82",
      "metadata": {
        "id": "50c9ac82"
      },
      "outputs": [],
      "source": [
        "class Identity(nn.Module):\n",
        "    \"\"\"Identity Module.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Squeeze(nn.Module):\n",
        "    \"\"\"Squeeze Module.\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.squeeze(self.dim)\n",
        "\n",
        "class SuperLinear(nn.Module):\n",
        "    \"\"\"SuperLinear Layer: Implements Neuron-Level Models (NLMs) for the CTM.\"\"\"\n",
        "    def __init__(self, in_dims, out_dims, N):\n",
        "        super().__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.register_parameter('w1', nn.Parameter(\n",
        "            torch.empty((in_dims, out_dims, N)).uniform_(\n",
        "                -1/math.sqrt(in_dims + out_dims),\n",
        "                 1/math.sqrt(in_dims + out_dims)\n",
        "            ), requires_grad=True)\n",
        "        )\n",
        "        self.register_parameter('b1', nn.Parameter(torch.zeros((1, N, out_dims)), requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "            out = torch.einsum('BDM,MHD->BDH', x, self.w1) + self.b1\n",
        "            out = out.squeeze(-1)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0eb50ea",
      "metadata": {
        "id": "b0eb50ea"
      },
      "source": [
        "Next, we define a helper function `compute_normalized_entropy`. We will use this function inside the CTM to compute the certainty of the model at each internal tick as `certainty = 1 - normalized entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4eedd9ee",
      "metadata": {
        "id": "4eedd9ee"
      },
      "outputs": [],
      "source": [
        "def compute_normalized_entropy(logits, reduction='mean'):\n",
        "    \"\"\"Computes the normalized entorpy for certainty-loss.\"\"\"\n",
        "    preds = F.softmax(logits, dim=-1)\n",
        "    log_preds = torch.log_softmax(logits, dim=-1)\n",
        "    entropy = -torch.sum(preds * log_preds, dim=-1)\n",
        "    num_classes = preds.shape[-1]\n",
        "    max_entropy = torch.log(torch.tensor(num_classes, dtype=torch.float32))\n",
        "    normalized_entropy = entropy / max_entropy\n",
        "    if len(logits.shape)>2 and reduction == 'mean':\n",
        "        normalized_entropy = normalized_entropy.flatten(1).mean(-1)\n",
        "    return normalized_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89b70a8",
      "metadata": {
        "id": "f89b70a8"
      },
      "source": [
        "## CTM Architecture Overview\n",
        "\n",
        "The CTM implementation is initialized with the following core parameters:\n",
        "\n",
        "- `iterations`: Number of internal ticks (recurrent steps).\n",
        "- `d_model`: Total number of neurons.\n",
        "- `d_input`: Input and attention embedding dimension.\n",
        "- `memory_length`: Length of the sliding activation window used by each neuron.\n",
        "- `heads`: Number of attention heads.\n",
        "- `n_synch_out`: Number of neurons used for output synchronization.\n",
        "- `n_synch_action`: Number of neurons used for computing attention queries.\n",
        "- `out_dims`: Dimensionality of the model's output.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "Upon initialization, the CTM builds the following modules:\n",
        "\n",
        "- **Backbone**: A CNN feature extractor for the input (e.g. image).\n",
        "- **Synapses**: A communication layer allowing neurons to interact.\n",
        "- **Trace Processor**: A neuron-level model that operates on each neuron's temporal activation trace.\n",
        "- **Synchronization Buffers**: For tracking decay.\n",
        "- **Learned Initial States**: Starting activations and traces for the system.\n",
        "\n",
        "---\n",
        "\n",
        "## Forward Pass Mechanics\n",
        "\n",
        "At each internal tick `stepi`, the CTM executes the following procedure:\n",
        "\n",
        "1. **Initialize recurrent state**:\n",
        "    - `state_trace`: Memory trace per neuron.\n",
        "    - `activated_state`: Current post-activations.\n",
        "    - `decay_alpha_out`, `decay_beta_out`: Values for calculating synchronization.\n",
        "\n",
        "2. **Featurize input**:\n",
        "    - Use the CNN backbone to extract key-value attention pairs `kv`.\n",
        "\n",
        "3. **Internal Loop** (for each tick `stepi`):\n",
        "    1. Compute `synchronisation_action` from `n_synch_action` neurons.\n",
        "    2. Generate attention query `q` from this synchronization.\n",
        "    3. Perform multi-head cross-attention over `kv`.\n",
        "    4. Concatenate attention output with the current neuron activations.\n",
        "    5. Update neurons via the synaptic module.\n",
        "    6. Append new activation to the trace window.\n",
        "    7. Update neuron states using the `trace_processor`.\n",
        "    8. Compute `synchronisation_out` from `n_synch_out` neurons.\n",
        "    9. Project to the output space via `output_projector`.\n",
        "    10. Compute prediction certainty from normalized entropy.\n",
        "\n",
        "This inner loop is repeated for the configured number of internal ticks. The CTM emits **predictions and certainties at every internal tick**.\n",
        "\n",
        "> For detailed mathematical formulation of the synchronization mechanism, please refer to the technical report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f357853f",
      "metadata": {
        "id": "f357853f"
      },
      "outputs": [],
      "source": [
        "class ContinuousThoughtMachine(nn.Module):\n",
        "    def __init__(self,\n",
        "                 iterations,\n",
        "                 d_model,\n",
        "                 d_input,\n",
        "                 memory_length,\n",
        "                 heads,\n",
        "                 n_synch_out,\n",
        "                 n_synch_action,\n",
        "                 out_dims,\n",
        "                 memory_hidden_dims,\n",
        "                 ):\n",
        "        super(ContinuousThoughtMachine, self).__init__()\n",
        "\n",
        "        # --- Core Parameters ---\n",
        "        self.iterations = iterations\n",
        "        self.d_model = d_model\n",
        "        self.d_input = d_input\n",
        "        self.memory_length = memory_length\n",
        "        self.n_synch_out = n_synch_out\n",
        "        self.n_synch_action = n_synch_action\n",
        "        self.out_dims = out_dims\n",
        "        self.memory_length = memory_length\n",
        "        self.memory_hidden_dims = memory_hidden_dims\n",
        "\n",
        "        # --- Input Processing  ---\n",
        "        self.backbone = nn.Sequential(\n",
        "             nn.LazyConv2d(d_input, kernel_size=3, stride=1, padding=1),\n",
        "             nn.BatchNorm2d(d_input),\n",
        "             nn.ReLU(),\n",
        "             nn.MaxPool2d(2, 2),\n",
        "             nn.LazyConv2d(d_input, kernel_size=3, stride=1, padding=1),\n",
        "             nn.BatchNorm2d(d_input),\n",
        "             nn.ReLU(),\n",
        "             nn.MaxPool2d(2, 2),\n",
        "         )\n",
        "       # self.backbone = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.attention = nn.MultiheadAttention(self.d_input, heads, batch_first=True)\n",
        "        self.kv_proj = nn.Sequential(nn.LazyLinear(self.d_input), nn.LayerNorm(self.d_input))\n",
        "        self.q_proj = nn.LazyLinear(self.d_input)\n",
        "\n",
        "        # --- Core CTM Modules ---\n",
        "        self.synapses = nn.Sequential(\n",
        "                nn.LazyLinear(d_model * 2),\n",
        "                nn.GLU(),\n",
        "                nn.LayerNorm(d_model)\n",
        "            )\n",
        "        self.trace_processor = nn.Sequential(\n",
        "            SuperLinear(in_dims=memory_length, out_dims=2 * memory_hidden_dims, N=d_model),\n",
        "            nn.GLU(),\n",
        "            SuperLinear(in_dims=memory_hidden_dims, out_dims=2, N=d_model),\n",
        "            nn.GLU(),\n",
        "            Squeeze(-1)\n",
        "        )\n",
        "\n",
        "        #  --- Start States ---\n",
        "        self.register_parameter('start_activated_state', nn.Parameter(\n",
        "                torch.zeros((d_model)).uniform_(-math.sqrt(1/(d_model)), math.sqrt(1/(d_model))),\n",
        "                requires_grad=True\n",
        "            ))\n",
        "\n",
        "        self.register_parameter('start_trace', nn.Parameter(\n",
        "            torch.zeros((d_model, memory_length)).uniform_(-math.sqrt(1/(d_model+memory_length)), math.sqrt(1/(d_model+memory_length))),\n",
        "            requires_grad=True\n",
        "        ))\n",
        "\n",
        "        # --- Synchronisation ---\n",
        "        self.synch_representation_size_action = (self.n_synch_action * (self.n_synch_action+1))//2\n",
        "        self.synch_representation_size_out = (self.n_synch_out * (self.n_synch_out+1))//2\n",
        "\n",
        "        for synch_type, size in [('action', self.synch_representation_size_action), ('out', self.synch_representation_size_out)]:\n",
        "            print(f\"Synch representation size {synch_type}: {size}\")\n",
        "\n",
        "        self.set_synchronisation_parameters('out', self.n_synch_out)\n",
        "        self.set_synchronisation_parameters('action', self.n_synch_action)\n",
        "\n",
        "        # --- Output Procesing ---\n",
        "        self.output_projector = nn.Sequential(nn.LazyLinear(self.out_dims))\n",
        "\n",
        "    def set_synchronisation_parameters(self, synch_type: str, n_synch: int):\n",
        "        left, right = self.initialize_left_right_neurons(synch_type, self.d_model, n_synch)\n",
        "        synch_representation_size = self.synch_representation_size_action if synch_type == 'action' else self.synch_representation_size_out\n",
        "        self.register_buffer(f'{synch_type}_neuron_indices_left', left)\n",
        "        self.register_buffer(f'{synch_type}_neuron_indices_right', right)\n",
        "        self.register_parameter(f'decay_params_{synch_type}', nn.Parameter(torch.zeros(synch_representation_size), requires_grad=True))\n",
        "\n",
        "    def initialize_left_right_neurons(self, synch_type, d_model, n_synch):\n",
        "        if synch_type == 'out':\n",
        "            neuron_indices_left = neuron_indices_right = torch.arange(0, n_synch)\n",
        "        elif synch_type == 'action':\n",
        "            neuron_indices_left = neuron_indices_right = torch.arange(d_model-n_synch, d_model)\n",
        "        return neuron_indices_left, neuron_indices_right\n",
        "\n",
        "    def compute_synchronisation(self, activated_state, decay_alpha, decay_beta, r, synch_type):\n",
        "        if synch_type == 'action':\n",
        "            n_synch = self.n_synch_action\n",
        "            selected_left = selected_right = activated_state[:, -n_synch:]\n",
        "        elif synch_type == 'out':\n",
        "            n_synch = self.n_synch_out\n",
        "            selected_left = selected_right = activated_state[:, :n_synch]\n",
        "\n",
        "        outer = selected_left.unsqueeze(2) * selected_right.unsqueeze(1)\n",
        "        i, j = torch.triu_indices(n_synch, n_synch)\n",
        "        pairwise_product = outer[:, i, j]\n",
        "\n",
        "        if decay_alpha is None or decay_beta is None:\n",
        "            decay_alpha = pairwise_product\n",
        "            decay_beta = torch.ones_like(pairwise_product)\n",
        "        else:\n",
        "            decay_alpha = r * decay_alpha + pairwise_product\n",
        "            decay_beta = r * decay_beta + 1\n",
        "\n",
        "        synchronisation = decay_alpha / (torch.sqrt(decay_beta))\n",
        "        return synchronisation, decay_alpha, decay_beta\n",
        "\n",
        "    def compute_features(self, x):\n",
        "        input_features = self.backbone(x)\n",
        "        kv = self.kv_proj(input_features.flatten(2).transpose(1, 2))\n",
        "        return kv\n",
        "\n",
        "    def compute_certainty(self, current_prediction):\n",
        "        ne = compute_normalized_entropy(current_prediction)\n",
        "        current_certainty = torch.stack((ne, 1-ne), -1)\n",
        "        return current_certainty\n",
        "\n",
        "    def forward(self, x, track=False):\n",
        "        B = x.size(0)\n",
        "        device = x.device\n",
        "\n",
        "        # --- Tracking Initialization ---\n",
        "        pre_activations_tracking = []\n",
        "        post_activations_tracking = []\n",
        "        synch_out_tracking = []\n",
        "        synch_action_tracking = []\n",
        "        attention_tracking = []\n",
        "\n",
        "        # --- Featurise Input Data ---\n",
        "        kv = self.compute_features(x)\n",
        "\n",
        "        # --- Initialise Recurrent State ---\n",
        "        state_trace = self.start_trace.unsqueeze(0).expand(B, -1, -1) # Shape: (B, H, T)\n",
        "        activated_state = self.start_activated_state.unsqueeze(0).expand(B, -1) # Shape: (B, H)\n",
        "\n",
        "        # --- Storage for outputs per iteration\n",
        "        predictions = torch.empty(B, self.out_dims, self.iterations, device=device, dtype=x.dtype)\n",
        "        certainties = torch.empty(B, 2, self.iterations, device=device, dtype=x.dtype)\n",
        "\n",
        "        decay_alpha_action, decay_beta_action = None, None\n",
        "        self.decay_params_action.data = torch.clamp(self.decay_params_action, 0, 15)  # Fix from github user: kuviki\n",
        "        self.decay_params_out.data = torch.clamp(self.decay_params_out, 0, 15)\n",
        "        r_action, r_out = torch.exp(-self.decay_params_action).unsqueeze(0).repeat(B, 1), torch.exp(-self.decay_params_out).unsqueeze(0).repeat(B, 1)\n",
        "\n",
        "        _, decay_alpha_out, decay_beta_out = self.compute_synchronisation(activated_state, None, None, r_out, synch_type='out')\n",
        "\n",
        "        # --- Recurrent Loop  ---\n",
        "        for stepi in range(self.iterations):\n",
        "\n",
        "            # --- Calculate Synchronisation for Input Data Interaction ---\n",
        "            synchronisation_action, decay_alpha_action, decay_beta_action = self.compute_synchronisation(activated_state, decay_alpha_action, decay_beta_action, r_action, synch_type='action')\n",
        "\n",
        "            # --- Interact with Data via Attention ---\n",
        "            q = self.q_proj(synchronisation_action).unsqueeze(1)\n",
        "            attn_out, attn_weights = self.attention(q, kv, kv, average_attn_weights=False, need_weights=True)\n",
        "            attn_out = attn_out.squeeze(1)\n",
        "            pre_synapse_input = torch.concatenate((attn_out, activated_state), dim=-1)\n",
        "\n",
        "            # --- Apply Synapses ---\n",
        "            state = self.synapses(pre_synapse_input)\n",
        "            state_trace = torch.cat((state_trace[:, :, 1:], state.unsqueeze(-1)), dim=-1)\n",
        "\n",
        "            # --- Activate ---\n",
        "            activated_state = self.trace_processor(state_trace)\n",
        "\n",
        "            # --- Calculate Synchronisation for Output Predictions ---\n",
        "            synchronisation_out, decay_alpha_out, decay_beta_out = self.compute_synchronisation(activated_state, decay_alpha_out, decay_beta_out, r_out, synch_type='out')\n",
        "\n",
        "            # --- Get Predictions and Certainties ---\n",
        "            current_prediction = self.output_projector(synchronisation_out)\n",
        "            current_certainty = self.compute_certainty(current_prediction)\n",
        "\n",
        "            predictions[..., stepi] = current_prediction\n",
        "            certainties[..., stepi] = current_certainty\n",
        "\n",
        "            # --- Tracking ---\n",
        "            if track:\n",
        "                pre_activations_tracking.append(state_trace[:,:,-1].detach().cpu().numpy())\n",
        "                post_activations_tracking.append(activated_state.detach().cpu().numpy())\n",
        "                attention_tracking.append(attn_weights.detach().cpu().numpy())\n",
        "                synch_out_tracking.append(synchronisation_out.detach().cpu().numpy())\n",
        "                synch_action_tracking.append(synchronisation_action.detach().cpu().numpy())\n",
        "\n",
        "        # --- Return Values ---\n",
        "        if track:\n",
        "            return predictions, certainties, (np.array(synch_out_tracking), np.array(synch_action_tracking)), np.array(pre_activations_tracking), np.array(post_activations_tracking), np.array(attention_tracking)\n",
        "        return predictions, certainties, synchronisation_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a049b6a",
      "metadata": {
        "id": "5a049b6a"
      },
      "source": [
        "## Certainty-Based Loss Function\n",
        "\n",
        "The CTM produces outputs at each internal tick, so the question arises: **how do we optimize the model across this internal temporal dimension?**\n",
        "\n",
        "Our answer is a simple but effective **certainty-based loss** that encourages the model to reason meaningfully across time. Instead of relying on the final tick alone, we aggregate loss from two key internal ticks:\n",
        "\n",
        "1. The tick where the **prediction loss** is lowest.\n",
        "2. The tick where the **certainty** (1 - normalized entropy) is highest.\n",
        "\n",
        "We then take the **average of the losses** at these two points.\n",
        "\n",
        "This approach encourages the CTM to both make accurate predictions and express high confidence in them—while supporting adaptive, interpretable computation over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0f463eb9",
      "metadata": {
        "id": "0f463eb9"
      },
      "outputs": [],
      "source": [
        "def get_loss(predictions, certainties, targets, use_most_certain=True):\n",
        "    \"\"\"use_most_certain will select either the most certain point or the final point.\"\"\"\n",
        "\n",
        "    losses = nn.CrossEntropyLoss(reduction='none')(predictions,\n",
        "                                                   torch.repeat_interleave(targets.unsqueeze(-1), predictions.size(-1), -1))\n",
        "\n",
        "    loss_index_1 = losses.argmin(dim=1)\n",
        "    loss_index_2 = certainties[:,1].argmax(-1)\n",
        "    if not use_most_certain:\n",
        "        loss_index_2[:] = -1\n",
        "\n",
        "    batch_indexer = torch.arange(predictions.size(0), device=predictions.device)\n",
        "    loss_minimum_ce = losses[batch_indexer, loss_index_1].mean()\n",
        "    loss_selected = losses[batch_indexer, loss_index_2].mean()\n",
        "\n",
        "    loss = (loss_minimum_ce + loss_selected)/2\n",
        "    return loss, loss_index_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e54afe0f",
      "metadata": {
        "id": "e54afe0f"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, targets, where_most_certain):\n",
        "    \"\"\"Calculate the accuracy based on the prediction at the most certain internal tick.\"\"\"\n",
        "    B = predictions.size(0)\n",
        "    device = predictions.device\n",
        "\n",
        "    predictions_at_most_certain_internal_tick = predictions.argmax(1)[torch.arange(B, device=device), where_most_certain].detach().cpu().numpy()\n",
        "    accuracy = (targets.detach().cpu().numpy() == predictions_at_most_certain_internal_tick).mean()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1371279",
      "metadata": {
        "id": "c1371279"
      },
      "outputs": [],
      "source": [
        "def prepare_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    train_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    test_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=1)\n",
        "    testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True, num_workers=1, drop_last=False)\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a492b058",
      "metadata": {
        "id": "a492b058"
      },
      "outputs": [],
      "source": [
        "def train(model, trainloader, testloader, iterations, device):\n",
        "\n",
        "  test_every = 100\n",
        "\n",
        "  optimizer = torch.optim.AdamW(params=list(model.parameters()), lr=0.0001, eps=1e-8)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  with tqdm(total=iterations, initial=0, dynamic_ncols=True) as pbar:\n",
        "      test_loss = None\n",
        "      test_accuracy = None\n",
        "      for stepi in range(iterations):\n",
        "\n",
        "          inputs, targets = next(iter(trainloader))\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          predictions, certainties, _ = model(inputs, track=False)\n",
        "          train_loss, where_most_certain = get_loss(predictions, certainties, targets)\n",
        "          train_accuracy = calculate_accuracy(predictions, targets, where_most_certain)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          train_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if stepi % test_every == 0:\n",
        "            model.eval()\n",
        "            with torch.inference_mode():\n",
        "                all_test_predictions = []\n",
        "                all_test_targets = []\n",
        "                all_test_where_most_certain = []\n",
        "                all_test_losses = []\n",
        "\n",
        "                for inputs, targets in testloader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    predictions, certainties, _ = model(inputs, track=False)\n",
        "                    test_loss, where_most_certain = get_loss(predictions, certainties, targets)\n",
        "                    all_test_losses.append(test_loss.item())\n",
        "\n",
        "                    all_test_predictions.append(predictions)\n",
        "                    all_test_targets.append(targets)\n",
        "                    all_test_where_most_certain.append(where_most_certain)\n",
        "\n",
        "                all_test_predictions = torch.cat(all_test_predictions, dim=0)\n",
        "                all_test_targets = torch.cat(all_test_targets, dim=0)\n",
        "                all_test_where_most_certain = torch.cat(all_test_where_most_certain, dim=0)\n",
        "\n",
        "                test_accuracy = calculate_accuracy(all_test_predictions, all_test_targets, all_test_where_most_certain)\n",
        "                test_loss = sum(all_test_losses) / len(all_test_losses)\n",
        "            model.train()\n",
        "\n",
        "          pbar.set_description(f'Train Loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.3f} Test Loss: {test_loss:.3f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "          pbar.update(1)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2d1658a9",
      "metadata": {
        "id": "2d1658a9",
        "outputId": "d7bee356-0655-4657-b191-f6de1c553477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synch representation size action: 136\n",
            "Synch representation size out: 136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Loss: 0.385, Train Accuracy: 0.859 Test Loss: 0.275, Test Accuracy: 0.886: 100%|██████████| 2000/2000 [11:54<00:00,  2.80it/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "trainloader, testloader = prepare_data()\n",
        "\n",
        "model = ContinuousThoughtMachine(\n",
        "    iterations = 30,\n",
        "    d_model = 128,\n",
        "    d_input = 32,\n",
        "    memory_length = 15,\n",
        "    heads = 1,\n",
        "    n_synch_out = 16,\n",
        "    n_synch_action = 16,\n",
        "    memory_hidden_dims = 8,\n",
        "    out_dims = 10,\n",
        "  ).to(device)\n",
        "\n",
        "\n",
        "model = train(model=model, trainloader=trainloader, testloader=testloader, iterations=2000, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kRXCl_vB-TOb"
      },
      "id": "kRXCl_vB-TOb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance on the orignal synpas Model\n",
        "\n",
        "Synch representation size action: 136\n",
        "Synch representation size out: 136\n",
        "<br>\n",
        "Train Loss: 0.247, Train Accuracy: 0.891 Test Loss: 0.248, Test Accuracy: 0.892: 100%|██████████| 2000/2000 [12:19<00:00,  2.70it/s]"
      ],
      "metadata": {
        "id": "BDFla7utReCv"
      },
      "id": "BDFla7utReCv"
    },
    {
      "cell_type": "markdown",
      "id": "af7e308f",
      "metadata": {
        "id": "af7e308f"
      },
      "source": [
        "## Visualizing CTM Dynamics\n",
        "\n",
        "We define a function to create GIFs that show how the CTMs dynamics. These visualizations include:\n",
        "\n",
        "- **Neuron activations** at each internal tick  \n",
        "- **Attention patterns** across the input  \n",
        "- **Predictions and certainty** at every step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fbae96",
      "metadata": {
        "id": "b3fbae96"
      },
      "outputs": [],
      "source": [
        "def make_gif(predictions, certainties, targets, pre_activations, post_activations, attention, inputs_to_model, filename):\n",
        "    def reshape_attention_weights(attention_weights):\n",
        "        T, B = attention_weights.shape[0], attention_weights.shape[1]\n",
        "        grid_size = math.sqrt(attention_weights.shape[-1])\n",
        "        assert grid_size.is_integer(), f'Grid size should be a perfect square, but got {attention_weights.shape[-1]}'\n",
        "        H_ATTENTION = W_ATTENTION = int(grid_size)\n",
        "        attn_weights_reshaped = attention_weights.reshape(T, B, -1, H_ATTENTION, W_ATTENTION)\n",
        "        return attn_weights_reshaped.mean(2)\n",
        "\n",
        "    batch_index = 0\n",
        "    n_neurons_to_visualise = 16\n",
        "    figscale = 0.28\n",
        "    n_steps = len(pre_activations)\n",
        "    heCTMap_cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "    frames = []\n",
        "\n",
        "    attention = reshape_attention_weights(attention)\n",
        "\n",
        "    these_pre_acts = pre_activations[:, batch_index, :]\n",
        "    these_post_acts = post_activations[:, batch_index, :]\n",
        "    these_inputs = inputs_to_model[batch_index,:, :, :]\n",
        "    these_attention_weights = attention[:, batch_index, :, :]\n",
        "    these_predictions = predictions[batch_index, :, :]\n",
        "    these_certainties = certainties[batch_index, :, :]\n",
        "    this_target = targets[batch_index]\n",
        "\n",
        "    class_labels = [str(i) for i in range(these_predictions.shape[0])]\n",
        "\n",
        "    mosaic = [['img_data', 'img_data', 'attention', 'attention', 'probs', 'probs', 'probs', 'probs'] for _ in range(2)] + \\\n",
        "             [['img_data', 'img_data', 'attention', 'attention', 'probs', 'probs', 'probs', 'probs'] for _ in range(2)] + \\\n",
        "             [['certainty'] * 8] + \\\n",
        "             [[f'trace_{ti}'] * 8 for ti in range(n_neurons_to_visualise)]\n",
        "\n",
        "    for stepi in range(n_steps):\n",
        "        fig_gif, axes_gif = plt.subplot_mosaic(mosaic=mosaic, figsize=(31*figscale*8/4, 76*figscale))\n",
        "        probs = softmax(these_predictions[:, stepi])\n",
        "        colors = [('g' if i == this_target else 'b') for i in range(len(probs))]\n",
        "\n",
        "        axes_gif['probs'].bar(np.arange(len(probs)), probs, color=colors, width=0.9, alpha=0.5)\n",
        "        axes_gif['probs'].set_title('Probabilities')\n",
        "        axes_gif['probs'].set_xticks(np.arange(len(probs)))\n",
        "        axes_gif['probs'].set_xticklabels(class_labels, fontsize=24)\n",
        "        axes_gif['probs'].set_yticks([])\n",
        "        axes_gif['probs'].tick_params(left=False, bottom=False)\n",
        "        axes_gif['probs'].set_ylim([0, 1])\n",
        "        for spine in axes_gif['probs'].spines.values():\n",
        "            spine.set_visible(False)\n",
        "        axes_gif['probs'].tick_params(left=False, bottom=False)\n",
        "        axes_gif['probs'].spines['top'].set_visible(False)\n",
        "        axes_gif['probs'].spines['right'].set_visible(False)\n",
        "        axes_gif['probs'].spines['left'].set_visible(False)\n",
        "        axes_gif['probs'].spines['bottom'].set_visible(False)\n",
        "\n",
        "        # Certainty\n",
        "        axes_gif['certainty'].plot(np.arange(n_steps), these_certainties[1], 'k-', linewidth=2)\n",
        "        axes_gif['certainty'].set_xlim([0, n_steps-1])\n",
        "        axes_gif['certainty'].axvline(x=stepi, color='black', linewidth=1, alpha=0.5)\n",
        "        axes_gif['certainty'].set_xticklabels([])\n",
        "        axes_gif['certainty'].set_yticklabels([])\n",
        "        axes_gif['certainty'].grid(False)\n",
        "        for spine in axes_gif['certainty'].spines.values():\n",
        "            spine.set_visible(False)\n",
        "\n",
        "        # Neuron Traces\n",
        "        for neuroni in range(n_neurons_to_visualise):\n",
        "            ax = axes_gif[f'trace_{neuroni}']\n",
        "            pre_activation = these_pre_acts[:, neuroni]\n",
        "            post_activation = these_post_acts[:, neuroni]\n",
        "            ax_pre = ax.twinx()\n",
        "\n",
        "            ax_pre.plot(np.arange(n_steps), pre_activation, color='grey', linestyle='--', linewidth=1, alpha=0.4)\n",
        "            color = 'blue' if neuroni % 2 else 'red'\n",
        "            ax.plot(np.arange(n_steps), post_activation, color=color, linewidth=2, alpha=1.0)\n",
        "\n",
        "            ax.set_xlim([0, n_steps-1])\n",
        "            ax_pre.set_xlim([0, n_steps-1])\n",
        "            ax.set_ylim([np.min(post_activation), np.max(post_activation)])\n",
        "            ax_pre.set_ylim([np.min(pre_activation), np.max(pre_activation)])\n",
        "\n",
        "            ax.axvline(x=stepi, color='black', linewidth=1, alpha=0.5)\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.grid(False)\n",
        "            ax_pre.set_xticklabels([])\n",
        "            ax_pre.set_yticklabels([])\n",
        "            ax_pre.grid(False)\n",
        "\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_visible(False)\n",
        "            for spine in ax_pre.spines.values():\n",
        "                spine.set_visible(False)\n",
        "\n",
        "        # Input image\n",
        "        this_image = these_inputs[0]\n",
        "        this_image = (this_image - this_image.min()) / (this_image.max() - this_image.min() + 1e-8)\n",
        "        axes_gif['img_data'].set_title('Input Image')\n",
        "        axes_gif['img_data'].imshow(this_image, cmap='binary', vmin=0, vmax=1)\n",
        "        axes_gif['img_data'].axis('off')\n",
        "\n",
        "        # Attention\n",
        "        this_input_gate = these_attention_weights[stepi]\n",
        "        gate_min, gate_max = np.nanmin(this_input_gate), np.nanmax(this_input_gate)\n",
        "        if not np.isclose(gate_min, gate_max):\n",
        "            normalized_gate = (this_input_gate - gate_min) / (gate_max - gate_min + 1e-8)\n",
        "        else:\n",
        "            normalized_gate = np.zeros_like(this_input_gate)\n",
        "        attention_weights_heCTMap = heCTMap_cmap(normalized_gate)[:,:,:3]\n",
        "\n",
        "        axes_gif['attention'].imshow(attention_weights_heCTMap, vmin=0, vmax=1)\n",
        "        axes_gif['attention'].axis('off')\n",
        "        axes_gif['attention'].set_title('Attention')\n",
        "\n",
        "        fig_gif.tight_layout()\n",
        "        canvas = fig_gif.canvas\n",
        "        canvas.draw()\n",
        "        image_numpy = np.frombuffer(canvas.buffer_rgba(), dtype='uint8')\n",
        "        image_numpy = image_numpy.reshape(*reversed(canvas.get_width_height()), 4)[:, :, :3]\n",
        "        frames.append(image_numpy)\n",
        "        plt.close(fig_gif)\n",
        "\n",
        "\n",
        "    mediapy.show_video(frames, width=400, codec=\"gif\")\n",
        "    imageio.mimsave(filename, frames, fps=5, loop=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20040db6",
      "metadata": {
        "id": "20040db6"
      },
      "source": [
        "The top row of the gif shows the input image, the attention weights and the models predictions.\n",
        "\n",
        "The second plot, in black, shows the models certainty over time.\n",
        "\n",
        "The red and blue lines correspond to the post-activations of different neurons, with the corresponding pre-activation shown in gray."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlRkd2Q0EGOe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    inputs, targets = next(iter(testloader))\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    predictions, certainties, (synch_out_tracking, synch_action_tracking), \\\n",
        "    pre_activations_tracking, post_activations_tracking, attention = model(inputs, track=True)\n",
        "\n",
        "    filename = \"mnist_output.gif\"\n",
        "\n",
        "    make_gif(\n",
        "        predictions.detach().cpu().numpy(),\n",
        "        certainties.detach().cpu().numpy(),\n",
        "        targets.detach().cpu().numpy(),\n",
        "        pre_activations_tracking,\n",
        "        post_activations_tracking,\n",
        "        attention,\n",
        "        inputs.detach().cpu().numpy(),\n",
        "        filename\n",
        "    )"
      ],
      "id": "BlRkd2Q0EGOe"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAes6cSbZH8O"
      },
      "id": "QAes6cSbZH8O",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}